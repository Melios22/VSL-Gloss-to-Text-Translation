{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4404e943-a078-4ee3-a496-f586dc19bab2",
    "_uuid": "fd8783fc-afbd-4e19-b71f-09a6e53d85f0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "___\n",
    "# 1. Download Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5f7f1760-195a-457a-885f-61fda29b383b",
    "_uuid": "56aab22c-445d-4cb6-bca1-ea7ef1d8ce0d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-13T11:56:52.340781Z",
     "iopub.status.busy": "2025-09-13T11:56:52.340349Z",
     "iopub.status.idle": "2025-09-13T11:57:06.752251Z",
     "shell.execute_reply": "2025-09-13T11:57:06.751182Z",
     "shell.execute_reply.started": "2025-09-13T11:56:52.340743Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%capture installation\n",
    "%pip install gdown\n",
    "# %pip install transformers\n",
    "# %pip install peft accelerate bitsandbytes triton\n",
    "%pip install evaluate sacrebleu rouge_score\n",
    "# !sudo apt install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3ad0a92b-0c0a-42b3-b679-0d8bbc341d41",
    "_uuid": "0c615348-eeee-4416-94cf-cab204f924bc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-13T11:57:06.754594Z",
     "iopub.status.busy": "2025-09-13T11:57:06.754299Z",
     "iopub.status.idle": "2025-09-13T11:57:14.089532Z",
     "shell.execute_reply": "2025-09-13T11:57:14.088463Z",
     "shell.execute_reply.started": "2025-09-13T11:57:06.754567Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# %%capture download_files\n",
    "!gdown 1fij-ftlExnP9X7l0Z0EvJHjRLzO1689c # train  https://drive.google.com/file/d/1fij-ftlExnP9X7l0Z0EvJHjRLzO1689c/view?usp=drive_link\n",
    "!gdown 1D8_p329vuN99_kPak-yZ9NUNKR8CS-hJ # val    https://drive.google.com/file/d/1D8_p329vuN99_kPak-yZ9NUNKR8CS-hJ/view?usp=drive_link\n",
    "!gdown 1XUNXKCX2KxroPekMEo59St7sx-STQM1g # test   https://drive.google.com/file/d/1XUNXKCX2KxroPekMEo59St7sx-STQM1g/view?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:57:14.091154Z",
     "iopub.status.busy": "2025-09-13T11:57:14.090895Z",
     "iopub.status.idle": "2025-09-13T11:57:50.548245Z",
     "shell.execute_reply": "2025-09-13T11:57:50.547207Z",
     "shell.execute_reply.started": "2025-09-13T11:57:14.091127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 2. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:43.166923Z",
     "iopub.status.busy": "2025-09-13T12:01:43.166391Z",
     "iopub.status.idle": "2025-09-13T12:01:43.213918Z",
     "shell.execute_reply": "2025-09-13T12:01:43.212743Z",
     "shell.execute_reply.started": "2025-09-13T12:01:43.166877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MODEL_NAME: str = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "# SRC_LANG: str = \"de_DE\"  # German\n",
    "# TGT_LANG: str = \"vi_VN\"  # Vietnamese\n",
    "# MODEL_NAME: str = \"Shahm/bart-german\"\n",
    "# MODEL_NAME: str = \"Helsinki-NLP/opus-mt-de-de\"\n",
    "# MODEL_NAME: str = \"Helsinki-NLP/opus-mt-de-vi\"\n",
    "MODEL_NAME: str = \"vinai/bartpho-word\"\n",
    "\n",
    "# Dataset - Optimized for German Gloss → Vietnamese\n",
    "MAX_INPUT_LENGTH: int = 128  # Increased for longer German gloss sequences\n",
    "MAX_TARGET_LENGTH: int = 128  # Increased for better Vietnamese sentence generation\n",
    "BATCH_SIZE: int = 12  # Optimized for T4 GPU memory\n",
    "\n",
    "# LoRA Configuration - Optimized for translation quality\n",
    "LORA_R: int = 32  # Higher rank for better capacity\n",
    "LORA_ALPHA: int = 64  # 2x rank for optimal scaling\n",
    "LORA_DROPOUT: float = 0.05  # Lower dropout for better learning\n",
    "LORA_TARGET_MODULES: list[str] = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"out_proj\",  # All attention layers\n",
    "    \"fc1\",\n",
    "    \"fc2\",  # Feed-forward layers for better translation\n",
    "]\n",
    "\n",
    "# Training Configuration\n",
    "OUTPUT_DIR = \"./gloss2vn_model\"  # More descriptive name\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Adjusted for larger batch size\n",
    "EPOCHS = 30  # More epochs for better convergence\n",
    "LEARNING_RATE = 5e-4  # Higher learning rate for faster convergence\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Optimization\n",
    "# USE_GRADIENT_CHECKPOINTING = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"CUDA devices available: {torch.cuda.device_count()}\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 3. Dataset Loader and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:57:50.565146Z",
     "iopub.status.busy": "2025-09-13T11:57:50.564827Z",
     "iopub.status.idle": "2025-09-13T11:57:51.172399Z",
     "shell.execute_reply": "2025-09-13T11:57:51.171421Z",
     "shell.execute_reply.started": "2025-09-13T11:57:50.565124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"PHOENIX-2014-T.train.csv\",\n",
    "        \"val\": \"PHOENIX-2014-T.dev.csv\",\n",
    "        \"test\": \"PHOENIX-2014-T.test.csv\",\n",
    "    },\n",
    "    delimiter=\"|\",\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:57:51.173768Z",
     "iopub.status.busy": "2025-09-13T11:57:51.173414Z",
     "iopub.status.idle": "2025-09-13T11:58:00.260534Z",
     "shell.execute_reply": "2025-09-13T11:58:00.259443Z",
     "shell.execute_reply.started": "2025-09-13T11:57:51.173744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# tokenizer.src_lang = SRC_LANG\n",
    "# tokenizer.tgt_lang = TGT_LANG\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Enhanced preprocessing for German gloss → Vietnamese translation\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for gloss, sentence in zip(examples[\"geGloss\"], examples[\"viSentence\"]):\n",
    "        # Clean and normalize German gloss\n",
    "        clean_gloss = gloss.strip(\" .\").replace(\"  \", \" \")  # Remove extra spaces\n",
    "        # Add special prefix to indicate gloss-to-text translation\n",
    "        # formatted_input = f\"German gloss: {clean_gloss}\"\n",
    "        formatted_input = clean_gloss\n",
    "\n",
    "        # Clean Vietnamese target\n",
    "        clean_target = sentence.strip(\" .\").replace(\"  \", \" \")\n",
    "\n",
    "        inputs.append(formatted_input)\n",
    "        targets.append(clean_target)\n",
    "\n",
    "    # Tokenize with improved settings\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False,  # Let data collator handle padding\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    # Tokenize targets with proper language setting\n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names, desc=\"Tokenizing datasets\"\n",
    ")\n",
    "\n",
    "# Data Collator will dynamically pad the batches\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=MODEL_NAME,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8,  # Optimize for tensor cores\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"val\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 4. Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:48.753819Z",
     "iopub.status.busy": "2025-09-13T12:01:48.753422Z",
     "iopub.status.idle": "2025-09-13T12:01:49.940428Z",
     "shell.execute_reply": "2025-09-13T12:01:49.939353Z",
     "shell.execute_reply.started": "2025-09-13T12:01:48.753791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Create optimized LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Ensure LoRA parameters require gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "model.train()\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:53.570427Z",
     "iopub.status.busy": "2025-09-13T12:01:53.569899Z",
     "iopub.status.idle": "2025-09-13T12:01:53.591856Z",
     "shell.execute_reply": "2025-09-13T12:01:53.590678Z",
     "shell.execute_reply.started": "2025-09-13T12:01:53.570393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# Optimized optimizer with better settings\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_params,\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-6,\n",
    ")\n",
    "\n",
    "# Advanced learning rate scheduler\n",
    "num_update_steps_per_epoch = len(train_dataloader) // GRADIENT_ACCUMULATION_STEPS\n",
    "num_training_steps = EPOCHS * num_update_steps_per_epoch\n",
    "num_warmup_steps = int(num_training_steps * WARMUP_RATIO)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "f\"Training steps: {num_training_steps}, Warmup: {num_warmup_steps}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 5. Metrics Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T11:58:12.414791Z",
     "iopub.status.busy": "2025-09-13T11:58:12.414074Z",
     "iopub.status.idle": "2025-09-13T11:58:16.202132Z",
     "shell.execute_reply": "2025-09-13T11:58:16.201050Z",
     "shell.execute_reply.started": "2025-09-13T11:58:12.414764Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "            # Enhanced generation for better quality\n",
    "            generated_tokens = model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                min_length=10,  # Ensure minimum translation length\n",
    "                num_beams=6,  # More beams for better search\n",
    "                length_penalty=1.2,  # Encourage longer, more complete translations\n",
    "                early_stopping=True,\n",
    "                do_sample=False,\n",
    "                repetition_penalty=1.1,  # Reduce repetition\n",
    "                no_repeat_ngram_size=3,  # Prevent 3-gram repetition\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                # forced_bos_token_id=tokenizer.lang_code_to_id[TGT_LANG], # BART mnt-mnt\n",
    "            )\n",
    "\n",
    "            labels = batch[\"labels\"]\n",
    "            labels = np.where(\n",
    "                labels.cpu() != -100, labels.cpu(), tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            post_preds, post_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "            all_preds.extend(post_preds)\n",
    "            all_labels.extend(post_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    bleu = bleu_metric.compute(predictions=all_preds, references=all_labels)\n",
    "    rouge = rouge_metric.compute(predictions=all_preds, references=all_labels)\n",
    "    chrf = chrf_metric.compute(predictions=all_preds, references=all_labels)\n",
    "\n",
    "    model.train()  # Set back to training mode\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu[\"score\"],\n",
    "        \"rougeL\": rouge[\"rougeL\"],\n",
    "        \"chrf\": chrf[\"score\"],\n",
    "        \"sample_pred\": all_preds[0] if all_preds else \"\",\n",
    "        \"sample_label\": all_labels[0][0] if all_labels else \"\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 6. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:01:57.704260Z",
     "iopub.status.busy": "2025-09-13T12:01:57.703219Z",
     "iopub.status.idle": "2025-09-13T12:01:57.717860Z",
     "shell.execute_reply": "2025-09-13T12:01:57.716173Z",
     "shell.execute_reply.started": "2025-09-13T12:01:57.704224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- SIMPLIFIED TRAINING LOOP FOR GERMAN GLOSS → VIETNAMESE ---\n",
    "import time\n",
    "\n",
    "best_bleu = float(\"-inf\")\n",
    "best_loss = float(\"inf\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "training_history = []\n",
    "\n",
    "print(f\"Starting training: {EPOCHS} epochs, {len(train_dataloader)} batches\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "        # Standard forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        # Update progress bar with current average loss\n",
    "        current_avg_loss = total_loss / (step + 1)\n",
    "        progress_bar.set_postfix({\"avg_loss\": f\"{current_avg_loss:.4f}\"})\n",
    "\n",
    "    # Epoch summary\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(model, eval_dataloader)\n",
    "    print(\n",
    "        f\"🎯Epoch {epoch + 1} - Metrics: BLEU={metrics['bleu']:.2f}, ROUGE-L={metrics['rougeL']:.2f}, chrF={metrics['chrf']:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Save training history\n",
    "    training_history.append(\n",
    "        {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_loss,\n",
    "            \"bleu\": metrics[\"bleu\"],\n",
    "            \"rouge\": metrics[\"rougeL\"],\n",
    "            \"chrf\": metrics[\"chrf\"],\n",
    "            \"time\": epoch_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save best model\n",
    "    if metrics[\"bleu\"] > best_bleu:\n",
    "        best_bleu = metrics[\"bleu\"]\n",
    "        print(f\"New best BLEU: {best_bleu:.2f} - Model saved\")\n",
    "\n",
    "        model.save_pretrained(OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "        import json\n",
    "\n",
    "        with open(f\"{OUTPUT_DIR}/training_history.json\", \"w\") as f:\n",
    "            json.dump(training_history, f, indent=2)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Training completed! Best BLEU: {best_bleu:.2f}\")\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T12:03:12.433429Z",
     "iopub.status.busy": "2025-09-13T12:03:12.432955Z",
     "iopub.status.idle": "2025-09-13T12:11:51.878513Z",
     "shell.execute_reply": "2025-09-13T12:11:51.876714Z",
     "shell.execute_reply.started": "2025-09-13T12:03:12.433389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the best model from training\n",
    "from peft import PeftModel\n",
    "\n",
    "best_model_path = OUTPUT_DIR\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, use_safetensors=True)\n",
    "final_model = PeftModel.from_pretrained(base_model, best_model_path).to(device)\n",
    "final_tokenizer = AutoTokenizer.from_pretrained(best_model_path)\n",
    "\n",
    "print(\"Evaluating best model on test set...\")\n",
    "test_metrics = evaluate_model(final_model, test_dataloader)\n",
    "print(f\"Test Results - BLEU: {test_metrics['bleu']:.3f}, ROUGE-L: {test_metrics['rougeL']:.3f}, chrF: {test_metrics['chrf']:.3f}\")\n",
    "print(f\"Model saved to: {best_model_path}\")\n",
    "# !tree -h {best_model_path}\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
